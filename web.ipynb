{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb9c1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=beec9e5b4c18d2ea8f65a39ca145fd893fbcb0e8a6f7d56794e6e6e8413fc6e3\n",
      "  Stored in directory: c:\\users\\saumya dabas\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ee595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting proxycrawl\n",
      "  Downloading proxycrawl-3.2.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: proxycrawl\n",
      "Successfully installed proxycrawl-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install proxycrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a928a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d605ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search keyword: cat\n",
      "Enter the number of images you want: 3\n",
      "Searching Images....\n",
      "Found 3 images\n",
      "Start downloading...\n",
      "Download Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests # pip install requests #to sent GET requests\n",
    "from bs4 import BeautifulSoup # pip install bs4 #to parse html(getting data out from html, xml or other markup languages)\n",
    "from proxycrawl.proxycrawl_api import ProxyCrawlAPI # pip install proxycrawl #for more details visit: https://proxycrawl.com/\n",
    "\n",
    "# download images from google search image URL\n",
    "Google_Image = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n",
    "\n",
    "Image_Folder = 'Google Images' # Creating a folder to save the images and assigning to a variable for further use\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(Image_Folder):\n",
    "        os.mkdir(Image_Folder)\n",
    "    download_images()\n",
    "\n",
    "def download_images():\n",
    "    data = input('Enter your search keyword: ')\n",
    "    num_images = int(input('Enter the number of images you want: '))\n",
    "    \n",
    "    print('Searching Images....')\n",
    "    \n",
    "    search_url = Google_Image + 'q=' + data #'q=' because its a query\n",
    "    \n",
    "    api = ProxyCrawlAPI({'token': 'f4Ohur6b7DCvfmZcDQHBVA'}) #Enter your ProxyCrawl Javascript token, you will get this after registering in https://proxycrawl.com/ for trail and paid\n",
    "    \n",
    "    response = api.get(search_url, {'scroll': 'true', 'scroll_interval': '60', 'ajax_wait': 'true'}) #Parameters of ProxyCrawl\n",
    "    if response['status_code'] == 200: #status code received from Google Site\n",
    "        b_soup = BeautifulSoup(response['body'], 'html.parser') #Body for ProxyCrawl token, #html.parser is used to parse/extract features from HTML files\n",
    "        results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n",
    "        \n",
    "        #extract the links of requested number of images with 'data-src' attribute and appended those links to a list 'imagelinks'\n",
    "    #allow to continue the loop in case query fails for non-data-src attributes        \n",
    "        count = 0\n",
    "        imagelinks= []\n",
    "        for res in results:\n",
    "            try:\n",
    "                link = res['data-src']\n",
    "                imagelinks.append(link)\n",
    "                count = count + 1\n",
    "                if (count >= num_images):\n",
    "                    break\n",
    "                \n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        print(f'Found {len(imagelinks)} images')\n",
    "        print('Start downloading...')\n",
    "    \n",
    "        for i, imagelink in enumerate(imagelinks):\n",
    "            response = requests.get(imagelink)\n",
    "            \n",
    "            # open each image link and save the file\n",
    "            imagename = Image_Folder + '/' + data + str(i+1) + '.jpg'\n",
    "            with open(imagename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "    \n",
    "        print('Download Completed!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed88cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
